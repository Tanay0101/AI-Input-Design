{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from Environment import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Direction mapping:\n",
    "0: left = [-1, 0]\n",
    "1: right = [1, 0]\n",
    "2: up = [0, -1]\n",
    "3: down = [0, 1]\n",
    "'''\n",
    "\n",
    "class User_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (4,))    #curr_x, curr_y, target_x, target_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(64, activation = 'relu')(x)\n",
    "        x = Dense(32, activation = 'relu')(x)\n",
    "        x = Dense(4)(x) #left, right, down, up\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "            \n",
    "\n",
    "    def play_one_step(self, env, state, mod_agent):\n",
    "        action_user = self.exp_policy(state)\n",
    "        action_user_one_hot = make_one_hot(action_user, 4)\n",
    "        curr_loc = state[:2]\n",
    "        target_loc = state[2:]\n",
    "        action_user_one_hot.extend(curr_loc)\n",
    "        mod_state = action_user_one_hot[:]\n",
    "        mod_state = np.array(mod_state)\n",
    "        new_loc, reward, done = mod_agent.play_one_step(env, mod_state, curr_loc, target_loc, self)\n",
    "        next_state = [new_loc[0], new_loc[1], target_loc[0], target_loc[1]]\n",
    "        self.replay_buffer.append(state, action_user, reward, next_state, done)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.model(next_states)\n",
    "        best_next_actions = np.argmax(next_Q_values, axis= 1)\n",
    "        next_mask = tf.one_hot(best_next_actions, 4).numpy()\n",
    "        max_next_Q_values = tf.reduce_sum(self.target_model(next_states)*next_mask, axis = 1, keepdims = True)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (6,))    #curr_x, curr_y, target_x, target_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(64, activation = 'relu')(x)\n",
    "        x = Dense(32, activation = 'relu')(x)\n",
    "        x = Dense(4)(x) #left, right, down, up\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(1,5)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])+1\n",
    "        \n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    def play_one_step(self, env, state, curr_loc, target_loc, user_agent):\n",
    "        #Agent not aware of target location\n",
    "        action_mod = self.exp_policy(state)\n",
    "        action_user = np.argmax(state[:4])\n",
    "        new_loc, reward_user, reward_mod, done = env.step(action_user, action_mod, target_loc, curr_loc)\n",
    "        next_dir = user_agent.exp_policy(np.array([new_loc[0], new_loc[1], target_loc[0], target_loc[1]]))\n",
    "        \n",
    "        next_dir_one_hot = make_one_hot(next_dir, 4)\n",
    "        next_dir_one_hot.extend(new_loc)\n",
    "        next_state = next_dir_one_hot[:]\n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        self.replay_buffer.append(state, action_mod-1, reward_mod, next_state, done)\n",
    "        \n",
    "        \n",
    "        return new_loc, reward_user, done\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.model(next_states)\n",
    "        best_next_actions = np.argmax(next_Q_values, axis= 1)\n",
    "        next_mask = tf.one_hot(best_next_actions, 4).numpy()\n",
    "        max_next_Q_values = tf.reduce_sum(self.target_model(next_states)*next_mask, axis = 1, keepdims = True)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icon Locations:\n",
      "[[0.  0.4]\n",
      " [0.  0.9]\n",
      " [0.  0.4]\n",
      " [0.2 0.7]\n",
      " [0.2 0.6]\n",
      " [0.1 0.9]]\n",
      "Icon usage Probabilities\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 4,484\n",
      "Trainable params: 4,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                224       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 4,548\n",
      "Trainable params: 4,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "user_agent = User_Agent()\n",
    "mod_agent = Mod_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 17/100000 [00:00<10:07, 164.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -4.0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 108/100000 [00:01<26:05, 63.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -4.58\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                            | 213/100000 [00:02<27:13, 61.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -4.73\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                            | 306/100000 [00:04<30:39, 54.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -5.54\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                            | 409/100000 [00:06<28:57, 57.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -4.91\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                            | 498/100000 [00:07<28:25, 58.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -4.9\n",
      "21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                            | 611/100000 [00:10<31:08, 53.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -3.86\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                            | 709/100000 [00:11<28:38, 57.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -4.44\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                            | 806/100000 [00:14<41:35, 39.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights\n",
      "Mean Reward = -3.89\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                            | 884/100000 [00:15<29:49, 55.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-426a070c9ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         user_agent.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmod_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-405ee8ba3715>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mall_Q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mQ_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_Q_values\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_Q_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    507\u001b[0m   \"\"\"\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6161\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   6162\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6163\u001b[1;33m         x, y)\n\u001b[0m\u001b[0;32m   6164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_agent.model.load_weights('user_agent.h5')\n",
    "rewards = []\n",
    "mean_rewards = []\n",
    "max_steps = 40\n",
    "reached = 0\n",
    "for epoch in tqdm(range(100000)):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    start, dest = env.give_start_dest()\n",
    "    state = [start[0], start[1], dest[0], dest[1]]\n",
    "    while not done and step<max_steps:\n",
    "        state = np.array(state)\n",
    "        next_state, reward, done = user_agent.play_one_step(env, state, mod_agent)\n",
    "        state = next_state\n",
    "        episode_reward+=reward\n",
    "        step+=1\n",
    "        if done:\n",
    "            reached+=1\n",
    "            \n",
    "    if epoch>50:\n",
    "#         user_agent.train()\n",
    "        mod_agent.train()\n",
    "    \n",
    "    if epoch>50 and epoch%100==0:\n",
    "#         user_agent.target_model.set_weights(user_agent.model.get_weights())\n",
    "        mod_agent.target_model.set_weights(mod_agent.model.get_weights())\n",
    "        print('Updated Weights')\n",
    "        \n",
    "    if epoch>50 and epoch%250==0:\n",
    "        mod_agent.epsilon*=0.9\n",
    "#         user_agent.epsilon*=0.9\n",
    "\n",
    "    mean_rewards.append(episode_reward)\n",
    "    if epoch%100==0:\n",
    "        rewards.append(np.mean(mean_rewards))\n",
    "        mean_rewards = []\n",
    "        print(f'Mean Reward = {rewards[-1]}')\n",
    "        print(reached)\n",
    "        reached = 0\n",
    "        \n",
    "    if epoch%500==0 and epoch:\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent.epsilon = 0\n",
    "done = False\n",
    "episode_reward = 0\n",
    "step = 0\n",
    "start, dest = env.give_start_dest()\n",
    "state = [start[0], start[1], dest[0], dest[1]]\n",
    "while not done and step<max_steps:\n",
    "    state = np.array(state)\n",
    "    print(state)\n",
    "    next_state, reward, done = user_agent.play_one_step(env, state, mod_agent)\n",
    "    state = next_state\n",
    "    episode_reward+=reward\n",
    "    step+=1\n",
    "    if done:\n",
    "        reached+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
