{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from Environment import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Direction mapping:\n",
    "0: left = [-1, 0]\n",
    "1: right = [1, 0]\n",
    "2: up = [0, -1]\n",
    "3: down = [0, 1]\n",
    "'''\n",
    "\n",
    "class User_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (4,))    #curr_x, curr_y, target_x, target_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(4, activation = 'softmax')(x) #left,right, down, up\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.005)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "\n",
    "    def play_one_step(self, env, state, mod_agent):\n",
    "        action_user = self.exp_policy(state)\n",
    "        action_user_one_hot = make_one_hot(action_user, 4)\n",
    "        curr_loc = state[:2]\n",
    "        target_loc = state[2:]\n",
    "        action_user_one_hot.extend(target_loc)\n",
    "        mod_state = action_user_one_hot[:]\n",
    "        mod_state = np.array(mod_state)\n",
    "        new_loc, reward, done = mod_agent.play_one_step(env, mod_state, curr_loc, target_loc, self)\n",
    "        next_state = [new_loc[0], new_loc[1], target_loc[0], target_loc[1]]\n",
    "        self.replay_buffer.append(state, action_user, reward, next_state, done)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.target_model(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (6,))   #direction of motion_one_hot(4), curr_x, curr_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(4, activation = 'softmax')(x) #modulate by 1,2,3,4 \n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.005)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 1\n",
    "        self.steps_per_epoch = 1\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    def play_one_step(self, env, state, curr_loc, target_loc, user_agent):\n",
    "        #Agent not aware of target location\n",
    "        action_mod = self.exp_policy(state)\n",
    "        action_user = state[0]\n",
    "        \n",
    "        new_loc, reward, done = env.step(action_user, action_mod, target_loc, curr_loc)\n",
    "        next_dir = user_agent.exp_policy(np.array([[new_loc[0], new_loc[1], target_loc[0], target_loc[1]]]))\n",
    "        \n",
    "        next_dir_one_hot = make_one_hot(next_dir, 4)\n",
    "        next_dir_one_hot.extend(target_loc)\n",
    "        next_state = next_dir_one_hot[:]\n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        self.replay_buffer.append(state, action_mod, reward, next_state, done)\n",
    "        \n",
    "        \n",
    "        return new_loc, reward, done\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.target_model(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icon Locations:\n",
      "[[0.4 0.9]\n",
      " [0.  0.2]\n",
      " [0.9 0.4]\n",
      " [0.4 0.2]\n",
      " [0.4 0.7]\n",
      " [0.5 0. ]]\n",
      "Icon usage Probabilities\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 292\n",
      "Trainable params: 292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                224       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 356\n",
      "Trainable params: 356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "user_agent = User_Agent()\n",
    "mod_agent = Mod_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -46.599999999999994\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                              | 16/1000 [00:00<00:21, 46.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.10000000000004\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▏                                                                             | 27/1000 [00:00<00:20, 48.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -118.75238095238095\n",
      "4\n",
      "Mean Reward = -117.33225806451614\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                            | 47/1000 [00:00<00:17, 54.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.29024390243906\n",
      "7\n",
      "Mean Reward = -108.46274509803922\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                           | 60/1000 [00:12<09:26,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.82295081967212\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▋                                                                          | 71/1000 [00:27<19:43,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.51830985915494\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▍                                                                         | 81/1000 [00:37<13:52,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.88888888888891\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▏                                                                        | 90/1000 [00:47<18:02,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.55274725274727\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 101/1000 [00:55<09:29,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -107.1653465346535\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▊                                                                      | 111/1000 [01:07<18:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.49549549549549\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▌                                                                     | 121/1000 [01:21<16:39,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.8603305785124\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▎                                                                    | 131/1000 [01:33<16:34,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.68931297709925\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▏                                                                   | 141/1000 [01:47<21:29,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.38652482269504\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▉                                                                   | 151/1000 [01:59<19:29,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.21920529801324\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▋                                                                  | 161/1000 [02:10<16:13,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.6913043478261\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 171/1000 [02:21<12:05,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.2058479532164\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▎                                                                | 181/1000 [02:31<15:30,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.08342541436463\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                | 191/1000 [02:41<09:26,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.7649214659686\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▉                                                               | 201/1000 [02:53<18:16,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.07910447761195\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████▋                                                              | 211/1000 [03:11<20:21,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.05165876777252\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▍                                                             | 221/1000 [03:23<14:39,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.85158371040724\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▏                                                            | 231/1000 [03:35<13:53,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.78658008658009\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████                                                            | 241/1000 [03:50<20:39,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.36348547717841\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▊                                                           | 251/1000 [04:01<08:33,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.12549800796813\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▌                                                          | 261/1000 [04:16<15:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.05632183908047\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▍                                                         | 271/1000 [04:28<13:22,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.67785977859779\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▏                                                        | 281/1000 [04:37<08:54,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.53024911032028\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████▉                                                        | 291/1000 [04:47<12:13,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.81890034364262\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▊                                                       | 301/1000 [04:54<12:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.38139534883722\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▌                                                      | 311/1000 [05:06<10:06,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.45176848874598\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▎                                                     | 321/1000 [05:20<15:06,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.40498442367601\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▏                                                    | 332/1000 [05:34<10:44,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.72416918429005\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 341/1000 [05:46<12:17,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.7108504398827\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████▋                                                   | 351/1000 [05:56<12:15,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.26011396011397\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▌                                                  | 361/1000 [06:10<17:08,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.02022160664819\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████▎                                                 | 371/1000 [06:24<16:06,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.59433962264151\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████                                                 | 381/1000 [06:38<12:48,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.1233595800525\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████▉                                                | 391/1000 [06:53<13:56,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.82915601023016\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▋                                               | 401/1000 [07:07<14:04,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.31720698254364\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████████████████████▍                                              | 411/1000 [07:20<10:26,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.54768856447689\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▎                                             | 421/1000 [07:33<11:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.5874109263658\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████                                             | 431/1000 [07:40<10:20,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.51067285382832\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████▊                                            | 441/1000 [07:51<11:36,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.30385487528345\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████▋                                           | 451/1000 [08:03<10:28,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.4490022172949\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▍                                          | 461/1000 [08:15<12:02,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.39783080260304\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████████▏                                         | 471/1000 [08:23<07:04,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.71125265392782\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████▉                                         | 481/1000 [08:36<12:23,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.87318087318087\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████████████████████████▊                                        | 491/1000 [08:49<12:18,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.19959266802444\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████▌                                       | 501/1000 [09:03<13:41,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.7572854291417\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████▎                                      | 510/1000 [09:14<10:23,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.53424657534246\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████▏                                     | 521/1000 [09:28<11:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.0397312859885\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████▉                                     | 531/1000 [09:43<11:14,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.59114877589454\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████▋                                    | 541/1000 [09:55<08:50,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.54584103512015\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████████▌                                   | 551/1000 [10:08<09:49,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.70344827586207\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████▍                                  | 562/1000 [10:22<08:09,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.11586452762924\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████                                  | 571/1000 [10:31<05:22,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.54203152364273\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████▉                                 | 581/1000 [10:44<08:32,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.89259896729777\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████▋                                | 591/1000 [10:57<09:19,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.0181049069374\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████▍                               | 601/1000 [11:12<10:46,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.52945091514145\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████▎                              | 611/1000 [11:25<08:27,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.69869067103112\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████                              | 621/1000 [11:38<07:59,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.86264090177136\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████▊                             | 631/1000 [11:50<07:42,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.7824088748019\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████▋                            | 641/1000 [12:01<08:20,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.61466458658346\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████▍                           | 651/1000 [12:12<06:49,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.4403993855607\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████▏                          | 661/1000 [12:25<07:47,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.63706505295008\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████                          | 671/1000 [12:39<07:43,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.96482861400895\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▉                         | 682/1000 [12:53<05:43,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.20616740088107\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████▌                        | 691/1000 [13:04<05:13,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.02387843704776\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████▍                       | 701/1000 [13:11<04:20,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.28131241084166\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████▏                      | 711/1000 [13:26<07:20,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.6748241912799\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████▉                      | 721/1000 [13:37<06:24,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.50748959778085\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████▋                     | 731/1000 [13:51<07:22,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -113.96990424076607\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████▌                    | 741/1000 [14:06<06:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.25910931174089\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████▎                   | 751/1000 [14:18<04:17,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.24993342210388\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████                   | 761/1000 [14:33<06:03,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.65072273324574\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████▉                  | 771/1000 [14:43<04:04,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.31232166018158\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████▋                 | 781/1000 [14:54<05:01,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.12650448143407\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████████████████▍                | 791/1000 [15:06<03:57,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.15461441213654\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████▎               | 801/1000 [15:20<04:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.43021223470663\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████               | 811/1000 [15:35<05:14,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.86658446362516\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████▊              | 821/1000 [15:48<03:32,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.97612667478685\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████▋             | 831/1000 [16:02<03:46,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.14861612515043\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████▍            | 841/1000 [16:14<03:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.16884661117717\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████▏           | 851/1000 [16:25<02:37,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.06615746180964\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████           | 861/1000 [16:40<03:50,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.3150987224158\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████▊          | 871/1000 [16:55<03:39,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.75820895522388\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████▌         | 881/1000 [17:09<02:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.9950056753689\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████▍        | 891/1000 [17:21<02:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.9132435465769\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████▏       | 901/1000 [17:36<02:41,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.18512763596004\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████▉       | 911/1000 [17:49<01:59,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.30263446761802\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████▊      | 922/1000 [18:03<01:02,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.50097719869706\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████▋     | 932/1000 [18:16<01:14,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.56659505907626\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████▎    | 941/1000 [18:26<00:55,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.30456960680128\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████████████████▏   | 952/1000 [18:41<00:57,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.57234490010516\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████▉   | 961/1000 [18:53<00:43,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.65119667013528\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████▋  | 971/1000 [19:07<00:40,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.77857878475798\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████▍ | 981/1000 [19:18<00:21,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.67186544342508\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████████▎| 991/1000 [19:35<00:15,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -117.14732593340061\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [19:46<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "mean_rewards = []\n",
    "max_steps = 200\n",
    "reached = 0\n",
    "for epoch in tqdm(range(1000)):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    while not done and step<max_steps:\n",
    "        start, dest = env.give_start_dest()\n",
    "        state = [start[0], start[1], dest[0], dest[1]]\n",
    "        state = np.array(state)\n",
    "        next_state, reward, done = user_agent.play_one_step(env, state, mod_agent)\n",
    "        state = next_state\n",
    "        episode_reward+=reward\n",
    "        step+=1\n",
    "        if done:\n",
    "            reached+=1\n",
    "        if epoch>50:\n",
    "            user_agent.train()\n",
    "            mod_agent.train()\n",
    "    \n",
    "    mean_rewards.append(episode_reward)\n",
    "    if epoch%10==0:\n",
    "        rewards.append(np.mean(mean_rewards))\n",
    "        print(f'Mean Reward = {rewards[-1]}')\n",
    "        print(reached)\n",
    "        reached = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
