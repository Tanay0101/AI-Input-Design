{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from Environment import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Direction mapping:\n",
    "0: left = [-1, 0]\n",
    "1: right = [1, 0]\n",
    "2: up = [0, -1]\n",
    "3: down = [0, 1]\n",
    "'''\n",
    "\n",
    "class User_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (4,))    #curr_x, curr_y, target_x, target_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(4, activation = 'softmax')(x) #left,right, down, up\n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.005)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 0.5\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "\n",
    "    def play_one_step(self, env, state, mod_agent):\n",
    "        action_user = self.exp_policy(state)\n",
    "        action_user_one_hot = make_one_hot(action_user, 4)\n",
    "        curr_loc = state[:2]\n",
    "        target_loc = state[2:]\n",
    "        action_user_one_hot.extend(target_loc)\n",
    "        mod_state = action_user_one_hot[:]\n",
    "        mod_state = np.array(mod_state)\n",
    "        new_loc, reward, done = mod_agent.play_one_step(env, mod_state, curr_loc, target_loc, self)\n",
    "        next_state = [new_loc[0], new_loc[1], target_loc[0], target_loc[1]]\n",
    "        self.replay_buffer.append(state, action_user, reward, next_state, done)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.target_model(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod_Agent:\n",
    "    def __init__(self):\n",
    "        #model\n",
    "        #-----------------------------------------------------\n",
    "        input_A = Input(shape = (6,))   #direction of motion_one_hot(4), curr_x, curr_y\n",
    "        x = Dense(32, activation = 'relu')(input_A)\n",
    "        x = Dense(4, activation = 'softmax')(x) #modulate by 1,2,3,4 \n",
    "        \n",
    "        self.model = Model(inputs = input_A, outputs = x)\n",
    "        print(self.model.summary())\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.005)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 0.5\n",
    "        self.steps_per_epoch = 1\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "    def exp_policy(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "    def sample_experience(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.state_history), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.replay_buffer.state_history[i] for i in indices])\n",
    "        actions = np.array([self.replay_buffer.action_history[i] for i in indices])\n",
    "        next_states = np.array([self.replay_buffer.next_state_history[i] for i in indices])\n",
    "        rewards = np.array([self.replay_buffer.rewards_history[i] for i in indices])\n",
    "        dones = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    def play_one_step(self, env, state, curr_loc, target_loc, user_agent):\n",
    "        #Agent not aware of target location\n",
    "        action_mod = self.exp_policy(state)\n",
    "        action_user = state[0]\n",
    "        \n",
    "        new_loc, reward, done = env.step(action_user, action_mod, target_loc, curr_loc)\n",
    "        next_dir = user_agent.exp_policy(np.array([[new_loc[0], new_loc[1], target_loc[0], target_loc[1]]]))\n",
    "        \n",
    "        next_dir_one_hot = make_one_hot(next_dir, 4)\n",
    "        next_dir_one_hot.extend(target_loc)\n",
    "        next_state = next_dir_one_hot[:]\n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        self.replay_buffer.append(state, action_mod, reward, next_state, done)\n",
    "        \n",
    "        \n",
    "        return new_loc, reward, done\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, next_states, rewards, dones = self.sample_experience()\n",
    "        next_Q_values = self.target_model(next_states)\n",
    "        max_next_Q_values = np.max(next_Q_values, axis= 1)\n",
    "        target_Q_values = rewards + (1-dones)*self.gamma*max_next_Q_values\n",
    "        \n",
    "        mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis = 1, keepdims = True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icon Locations:\n",
      "[[0.6 0.4]\n",
      " [0.3 0.7]\n",
      " [0.  0. ]\n",
      " [0.4 0.3]\n",
      " [0.3 0.7]\n",
      " [0.2 0.7]]\n",
      "Icon usage Probabilities\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 292\n",
      "Trainable params: 292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                224       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 356\n",
      "Trainable params: 356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "user_agent = User_Agent()\n",
    "mod_agent = Mod_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                | 2/1000 [00:00<01:18, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = 8.9\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                               | 12/1000 [00:02<03:11,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.5181818181818\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                              | 21/1000 [00:03<02:54,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -104.67142857142856\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                             | 31/1000 [00:06<04:21,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -121.36774193548388\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                            | 41/1000 [00:08<03:57,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -123.32439024390244\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                            | 50/1000 [00:11<03:16,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -120.39999999999999\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                           | 61/1000 [00:26<25:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.38196721311479\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▋                                                                          | 71/1000 [00:46<23:39,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -116.28732394366196\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▍                                                                         | 81/1000 [01:01<23:15,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.30987654320988\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                        | 91/1000 [01:17<26:11,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.07032967032967\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 101/1000 [01:33<22:54,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.89207920792082\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▊                                                                      | 111/1000 [01:49<26:35,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.19189189189188\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▌                                                                     | 121/1000 [02:04<25:02,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -114.96198347107438\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▎                                                                    | 131/1000 [02:21<24:50,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.57022900763359\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▏                                                                   | 141/1000 [02:37<26:58,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -115.81985815602836\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▉                                                                   | 151/1000 [02:44<08:03,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.41655629139073\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▋                                                                  | 161/1000 [03:00<23:08,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.92795031055901\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 171/1000 [03:15<21:52,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.41988304093569\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▎                                                                | 181/1000 [03:28<22:54,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.41546961325969\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                | 191/1000 [03:48<26:09,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -112.60314136125653\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▉                                                               | 201/1000 [03:58<10:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.56169154228857\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████▋                                                              | 211/1000 [04:13<17:39,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -110.31611374407585\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▍                                                             | 221/1000 [04:27<19:22,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.85656108597287\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▎                                                            | 232/1000 [04:48<19:11,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -111.8809523809524\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████                                                            | 241/1000 [04:59<12:48,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.8871369294606\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▊                                                           | 251/1000 [05:12<23:26,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.57211155378485\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▌                                                          | 261/1000 [05:27<12:28,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.52988505747128\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▍                                                         | 271/1000 [05:41<15:36,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.32250922509226\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▏                                                        | 281/1000 [05:55<15:41,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.8202846975089\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████▉                                                        | 291/1000 [06:09<12:38,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.72405498281789\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▊                                                       | 301/1000 [06:24<13:53,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.71029900332226\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▌                                                      | 311/1000 [06:36<10:29,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.12861736334405\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▎                                                     | 321/1000 [06:53<19:17,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.58909657320874\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▏                                                    | 331/1000 [07:07<11:42,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.41903323262841\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 341/1000 [07:21<15:41,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -108.35014662756599\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████▋                                                   | 351/1000 [07:41<22:18,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.68717948717948\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▌                                                  | 361/1000 [07:54<14:04,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.32437673130195\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████▎                                                 | 371/1000 [08:11<19:36,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward = -109.60080862533692\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████                                                 | 380/1000 [08:25<19:32,  1.89s/it]"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "mean_rewards = []\n",
    "max_steps = 200\n",
    "reached = 0\n",
    "for epoch in tqdm(range(1000)):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    while not done and step<max_steps:\n",
    "        start, dest = env.give_start_dest()\n",
    "        state = [start[0], start[1], dest[0], dest[1]]\n",
    "        state = np.array(state)\n",
    "        next_state, reward, done = user_agent.play_one_step(env, state, mod_agent)\n",
    "        state = next_state\n",
    "        episode_reward+=reward\n",
    "        step+=1\n",
    "        if done:\n",
    "            reached+=1\n",
    "        if epoch>50:\n",
    "            user_agent.train()\n",
    "            mod_agent.train()\n",
    "            mod_agent.epsilon*=0.9\n",
    "            user_agent.epsilon*=0.9\n",
    "            \n",
    "    mean_rewards.append(episode_reward)\n",
    "    if epoch%10==0:\n",
    "        rewards.append(np.mean(mean_rewards))\n",
    "        print(f'Mean Reward = {rewards[-1]}')\n",
    "        print(reached)\n",
    "        reached = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
