{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from Environment import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "dense_1_user =  Dense(32, activation = 'relu')\n",
    "dense_2_user =  Dense(32, activation = 'relu')\n",
    "# dense_3_user =  Dense(32, activation = 'relu')\n",
    "\n",
    "dense_1_assist =  Dense(32, activation = 'relu')\n",
    "lstm_1_assist = LSTM(32, activation = 'relu')\n",
    "# dense_2_assist = Dense(32, activation = 'relu')\n",
    "\n",
    "advantage_layer_user = Dense(4)\n",
    "value_layer_user = Dense(1)\n",
    "\n",
    "advantage_layer_assist = Dense(4)\n",
    "value_layer_assist = Dense(1)\n",
    "\n",
    "advantage_layer = Dense(4)\n",
    "value_layer = Dense(1)\n",
    "\n",
    "advantage_layer_user = advantage_layer\n",
    "advantage_layer_assist = advantage_layer\n",
    "\n",
    "value_layer_user = value_layer\n",
    "value_layer_assist = value_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Design:\n",
    "    def __init__(self, steps = 4):        \n",
    "        self.loss_fn = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "        self.batch_size = 128\n",
    "        self.replay_buffer_size = 1024\n",
    "        self.replay_buffer = Replay_Buffer(self.replay_buffer_size)\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.9\n",
    "        self.env = Environment()\n",
    "        self.env.cells = np.array([[0.7, 0.1], [0.1, 0.1], [0.5, 0.7], [0.6, 0.2], [0.7, 0.4], [0.2, 0.9]])\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        input_A = Input(shape = (4,))\n",
    "        input_B = Input(shape = (steps,6))\n",
    "        action_user = Input(shape = 1, dtype = tf.int32)\n",
    "        action_assist = Input(shape = 1, dtype = tf.int32)\n",
    "        \n",
    "        x = Subtract()([input_A[:, 2:], input_A[:, :2]])\n",
    "        x = dense_1_user(x)\n",
    "        x = dense_2_user(x)\n",
    "#         x = dense_3_user(x)\n",
    "        adv_user = advantage_layer_user(x)\n",
    "        val_user = value_layer_user(x)\n",
    "        output_user = adv_user - tf.reduce_mean(adv_user, axis = 1, keepdims = True) + val_user\n",
    "        \n",
    "        self.user_model = Model(inputs = input_A, outputs = output_user)\n",
    "        self.user_model.summary()\n",
    "        \n",
    "        self.target_user_model = tf.keras.models.clone_model(self.user_model)\n",
    "        self.target_user_model.set_weights(self.user_model.get_weights())\n",
    "        \n",
    "\n",
    "        \n",
    "        y = dense_1_assist(input_B)\n",
    "        y = lstm_1_assist(y)\n",
    "#         y = dense_2_assist(y)\n",
    "        adv_assist = advantage_layer_assist(y)\n",
    "        val_assist = value_layer_assist(y)\n",
    "        output_assist = adv_assist - tf.reduce_mean(adv_assist, axis = 1, keepdims = True) + val_assist\n",
    "        \n",
    "        self.assist_model = Model(inputs = input_B, outputs = output_assist)\n",
    "        self.assist_model.summary()\n",
    "        \n",
    "        self.target_assist_model = tf.keras.models.clone_model(self.assist_model)\n",
    "        self.target_assist_model.set_weights(self.assist_model.get_weights())\n",
    "        \n",
    "        mask_user = tf.reduce_sum(tf.one_hot(action_user, 4), axis = 1)\n",
    "        mask_assist = tf.reduce_sum(tf.one_hot(action_assist, 4), axis = 1)\n",
    "        output_user = output_user*mask_user\n",
    "        output_assist = output_assist*mask_assist\n",
    "        \n",
    "        out = tf.reduce_sum(output_user + output_assist, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.model = Model(inputs = [input_A, input_B, action_user, action_assist], outputs = out)  \n",
    "        self.model.summary() \n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def infer(self):\n",
    "        ob_user, action_user, reward_user, next_ob_user, ob_assist, action_assist,\\\n",
    "        reward_assist, next_ob_assist, done = self.sample_exp()\n",
    "        \n",
    "        ob_user = ob_user[1:4]\n",
    "        action_user = action_user[1:4]\n",
    "        reward_user = reward_user[1:4]\n",
    "        \n",
    "        ob_assist = ob_assist[1:4]\n",
    "        action_assist = action_assist[1:4]\n",
    "        reward_assist = reward_assist[1:4]\n",
    "        \n",
    "        print(action_user, action_assist)\n",
    "        \n",
    "        print(self.user_model(ob_user))\n",
    "        print(self.assist_model(ob_assist))\n",
    "        \n",
    "        print(self.model([ob_user, ob_assist, action_user, action_assist]))\n",
    "    \n",
    "    def exp_policy_user(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.user_model(state)\n",
    "            return np.argmax(Q_values[0])\n",
    "    \n",
    "    def exp_policy_assist(self, state):\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.randint(1,5)\n",
    "        else:\n",
    "            state = np.array(state)[np.newaxis]\n",
    "            Q_values = self.assist_model(state)\n",
    "            return np.argmax(Q_values[0])+1\n",
    "    \n",
    "    def step(self, ob_user, prev_steps_assist):\n",
    "        curr_loc = ob_user[:2]\n",
    "        target_loc = ob_user[2:4]\n",
    "        \n",
    "        action_user = self.exp_policy_user(ob_user)\n",
    "        action_user_one_hot = make_one_hot(action_user, 4)\n",
    "        \n",
    "        ob_assist = [action_user_one_hot + ob_user[:2]]\n",
    "        ob_assist = prev_steps_assist + ob_assist \n",
    "        action_assist = self.exp_policy_assist(ob_assist)\n",
    "        \n",
    "        new_loc, reward_user, reward_assist, done = self.env.step(action_user, action_assist-1, target_loc, curr_loc)\n",
    "        \n",
    "        next_ob_user = new_loc[:]\n",
    "        next_ob_user = next_ob_user + target_loc\n",
    "        \n",
    "        next_action_user = self.exp_policy_user(next_ob_user)\n",
    "        next_action_user_one_hot = make_one_hot(next_action_user, 4)\n",
    "        next_ob_assist = [next_action_user_one_hot + next_ob_user[:2]]\n",
    "        next_ob_assist = ob_assist[1:] + next_ob_assist\n",
    "        \n",
    "        self.add_replay_buffer(ob_user, action_user, reward_user, next_ob_user, ob_assist,\\\n",
    "                          action_assist-1, reward_assist, next_ob_assist, done)\n",
    "        \n",
    "        return next_ob_user, ob_assist[1:], reward_user, reward_assist, done \n",
    "        \n",
    "        \n",
    "    \n",
    "    def add_replay_buffer(self, ob_user, action_user, reward_user, next_ob_user, ob_assist,\\\n",
    "                         action_assist, reward_assist, next_ob_assist, done):\n",
    "        \n",
    "        self.replay_buffer.ob_user_history.append(ob_user)\n",
    "        self.replay_buffer.action_user_history.append(action_user)\n",
    "        self.replay_buffer.reward_user_history.append(reward_user)\n",
    "        self.replay_buffer.next_ob_user_history.append(next_ob_user)\n",
    "        self.replay_buffer.ob_assist_history.append(ob_assist)\n",
    "        self.replay_buffer.action_assist_history.append(action_assist)\n",
    "        self.replay_buffer.reward_assist_history.append(reward_assist)\n",
    "        self.replay_buffer.next_ob_assist_history.append(next_ob_assist)\n",
    "        self.replay_buffer.done_history.append(done)\n",
    "    \n",
    "    def sample_exp(self):\n",
    "        indices = np.random.randint(len(self.replay_buffer.done_history), size = self.batch_size)\n",
    "        \n",
    "        ob_user = np.array([self.replay_buffer.ob_user_history[i] for i in indices])\n",
    "        action_user = np.array([self.replay_buffer.action_user_history[i] for i in indices])\n",
    "        reward_user = np.array([self.replay_buffer.reward_user_history[i] for i in indices])\n",
    "        next_ob_user = np.array([self.replay_buffer.next_ob_user_history[i] for i in indices])\n",
    "        ob_assist = np.array([self.replay_buffer.ob_assist_history[i] for i in indices])\n",
    "        action_assist = np.array([self.replay_buffer.action_assist_history[i] for i in indices])\n",
    "        reward_assist = np.array([self.replay_buffer.reward_assist_history[i] for i in indices])\n",
    "        next_ob_assist = np.array([self.replay_buffer.next_ob_assist_history[i] for i in indices])\n",
    "        done = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return ob_user, action_user, reward_user, next_ob_user, ob_assist, action_assist, reward_assist, next_ob_assist, done \n",
    "    \n",
    "    def train(self):\n",
    "        ob_user, action_user, reward_user, next_ob_user, ob_assist, action_assist,\\\n",
    "        reward_assist, next_ob_assist, done = self.sample_exp()\n",
    "        \n",
    "        input_A = ob_user\n",
    "        input_B = ob_assist\n",
    "        \n",
    "        rewards = reward_user + reward_assist\n",
    "        \n",
    "        next_Q_values_user, next_Q_values_assist = self.user_model(next_ob_user), self.assist_model(next_ob_assist)\n",
    "        best_next_actions_user, best_next_actions_assist = tf.math.argmax(next_Q_values_user, axis = 1), tf.math.argmax(next_Q_values_assist, axis = 1)\n",
    "        next_Q_values_user, next_Q_values_assist = self.target_user_model(next_ob_user), self.target_assist_model(next_ob_assist)\n",
    "        \n",
    "        best_next_Q_values_user = tf.reduce_sum(next_Q_values_user*tf.one_hot(best_next_actions_user, 4), axis = 1)\n",
    "        best_next_Q_values_assist = tf.reduce_sum(next_Q_values_user*tf.one_hot(best_next_actions_assist, 4), axis = 1)\n",
    "        best_next_Q_values = best_next_Q_values_user + best_next_Q_values_assist\n",
    "        \n",
    "        target_Q_values = rewards + (1-done)*self.gamma*best_next_Q_values\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values = self.model([input_A, input_B, action_user, action_assist])\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "        \n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        Q_values_assist = self.assist_model(input_B)\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values_user = self.user_model(input_A)\n",
    "            loss = tf.reduce_mean(self.loss_fn(Q_values_assist, Q_values_user))\n",
    "            \n",
    "        grads = tape.gradient(loss, self.user_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.user_model.trainable_variables))\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_values_assist = self.assist_model(input_B)\n",
    "            loss = tf.reduce_mean(self.loss_fn(Q_values_assist, Q_values_user))\n",
    "            \n",
    "        grads = tape.gradient(loss, self.assist_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.assist_model.trainable_variables))                          \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icon Locations:\n",
      "[[0.1 0.6]\n",
      " [0.1 0.1]\n",
      " [0.2 0.5]\n",
      " [0.6 0. ]\n",
      " [0.1 0.4]\n",
      " [0.  0.9]]\n",
      "Icon usage Probabilities\n",
      "[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 2)]          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 2)]          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 2)            0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           96          subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            132         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 4)]          0           dense_7[0][0]                    \n",
      "                                                                 tf_op_layer_Mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 4)]          0           tf_op_layer_Sub[0][0]            \n",
      "                                                                 dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,317\n",
      "Trainable params: 1,317\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 4, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4, 32)        224         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 32)           8320        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            132         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_1 (TensorFlowO [(None, 1)]          0           dense_7[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 4)]          0           dense_7[1][0]                    \n",
      "                                                                 tf_op_layer_Mean_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            33          lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, 4)]          0           tf_op_layer_Sub_1[0][0]          \n",
      "                                                                 dense_8[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,709\n",
      "Trainable params: 8,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 2)]          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 2)]          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 2)            0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           96          subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4, 32)        224         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 32)           8320        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            132         dense_1[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_1 (TensorFlowO [(None, 1)]          0           dense_7[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 4)]          0           dense_7[0][0]                    \n",
      "                                                                 tf_op_layer_Mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_OneHot (TensorFlowO [(None, 1, 4)]       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 4)]          0           dense_7[1][0]                    \n",
      "                                                                 tf_op_layer_Mean_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_OneHot_1 (TensorFlo [(None, 1, 4)]       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 4)]          0           tf_op_layer_Sub[0][0]            \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 4)]          0           tf_op_layer_OneHot[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, 4)]          0           tf_op_layer_Sub_1[0][0]          \n",
      "                                                                 dense_8[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None, 4)]          0           tf_op_layer_OneHot_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 4)]          0           tf_op_layer_AddV2[0][0]          \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 4)]          0           tf_op_layer_AddV2_1[0][0]        \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_2 (TensorFlow [(None, 4)]          0           tf_op_layer_Mul[0][0]            \n",
      "                                                                 tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 1)]          0           tf_op_layer_AddV2_2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 9,861\n",
      "Trainable params: 9,861\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "steps = 4\n",
    "model = AI_Design(steps)\n",
    "env = model.env\n",
    "\n",
    "\n",
    "# if os.path.exists('user_model.h5'):\n",
    "#     model.user_model = tf.keras.models.load_model('user_model.h5')\n",
    "#     model.assist_model = tf.keras.models.load_model('assist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_prev_steps(prev_steps_assist, steps):\n",
    "    prev_steps_assist = [[0,0,0,0,-1,-1] for i in range(steps-1)]\n",
    "    return prev_steps_assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 102/100000 [00:06<3:06:47,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                          | 502/100000 [00:45<2:44:14, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -32.41778122504205\n",
      "Successful runs = 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                          | 601/100000 [00:54<2:54:54,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                         | 1001/100000 [01:32<2:40:44, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -30.641966614442545\n",
      "Successful runs = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                                         | 1503/100000 [02:27<2:58:53,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -31.112644621492922\n",
      "Successful runs = 13.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                        | 1602/100000 [02:39<3:17:22,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                        | 2002/100000 [03:23<3:29:53,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -32.725387275470105\n",
      "Successful runs = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▊                                                                        | 2502/100000 [04:41<4:18:08,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -29.91318328153107\n",
      "Successful runs = 16.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▏                                                                       | 3001/100000 [06:03<4:27:27,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -30.735659997900022\n",
      "Successful runs = 14.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▌                                                                       | 3501/100000 [07:31<5:37:42,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Saved Weights\n",
      "Running reward = -28.29327190681303\n",
      "Successful runs = 16.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                       | 4002/100000 [08:59<4:34:41,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -27.690108583965888\n",
      "Successful runs = 17.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███                                                                       | 4201/100000 [09:35<5:21:40,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▎                                                                      | 4502/100000 [10:30<5:07:15,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -28.170246288479323\n",
      "Successful runs = 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▋                                                                      | 5002/100000 [12:01<4:33:26,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -30.95385432649238\n",
      "Successful runs = 17.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████                                                                      | 5502/100000 [13:41<4:57:58,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Saved Weights\n",
      "Running reward = -26.107164742038314\n",
      "Successful runs = 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▏                                                                     | 5701/100000 [14:22<6:05:16,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▎                                                                     | 5801/100000 [14:42<6:02:44,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▍                                                                     | 6002/100000 [15:24<5:22:53,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -29.214718769660085\n",
      "Successful runs = 21.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████▊                                                                     | 6501/100000 [17:14<5:55:13,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -26.042083219641164\n",
      "Successful runs = 20.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▏                                                                    | 7001/100000 [19:01<6:10:39,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -28.317113414938802\n",
      "Successful runs = 21.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▌                                                                    | 7501/100000 [20:54<6:04:36,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -28.707388364225217\n",
      "Successful runs = 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▉                                                                    | 8001/100000 [22:46<5:35:10,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -27.091445496690376\n",
      "Successful runs = 20.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▎                                                                   | 8501/100000 [24:49<6:06:04,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -29.27101308375608\n",
      "Successful runs = 20.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▋                                                                   | 9001/100000 [27:02<6:52:21,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -27.807565312665627\n",
      "Successful runs = 21.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▉                                                                   | 9401/100000 [28:58<9:48:57,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Saved Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████                                                                   | 9502/100000 [29:25<5:45:25,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -22.690843246249386\n",
      "Successful runs = 26.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▎                                                                 | 10001/100000 [31:44<7:25:56,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward = -29.363942007591753\n",
      "Successful runs = 16.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▍                                                                 | 10180/100000 [32:37<7:11:57,  3.47it/s]"
     ]
    }
   ],
   "source": [
    "max_steps = 40\n",
    "reached = 0\n",
    "reached_history = []\n",
    "max_reached = 0\n",
    "\n",
    "running_reward = 0\n",
    "\n",
    "for epoch in tqdm(range(100000)):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    start, dest = env.give_start_dest()\n",
    "    ob_user = [start[0], start[1], dest[0], dest[1]]\n",
    "    prev_steps_assist = []\n",
    "    prev_steps_assist = give_prev_steps(prev_steps_assist, steps)\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step<max_steps:\n",
    "        ob_user, prev_steps_assist, reward_user, reward_assist, done = model.step(ob_user, prev_steps_assist)\n",
    "        episode_reward+=reward_user\n",
    "        step+=1\n",
    "        if done:\n",
    "            reached+=1\n",
    "    \n",
    "    if epoch:\n",
    "        running_reward = 0.01 * episode_reward + (1 - 0.01) * running_reward\n",
    "    else:\n",
    "        running_reward = episode_reward\n",
    "        \n",
    "    if epoch>50:\n",
    "        model.train()\n",
    "        \n",
    "        if epoch%100==0:\n",
    "            model.target_user_model.set_weights(model.user_model.get_weights())\n",
    "            model.target_assist_model.set_weights(model.assist_model.get_weights())\n",
    "            reached_history.append(reached)\n",
    "            rewards = []\n",
    "            \n",
    "            if reached>max_reached:\n",
    "                print(reached)\n",
    "                print('Saved Weights')\n",
    "                max_reached = reached\n",
    "                model.user_model.save('user_model.h5')\n",
    "                model.assist_model.save('assist_model.h5')\n",
    "                \n",
    "            reached = 0\n",
    "            \n",
    "            if epoch%500==0:\n",
    "                print(f'Running reward = {running_reward}')\n",
    "                print(f'Successful runs = {np.mean(reached_history)}')\n",
    "                reached_history = []\n",
    "                \n",
    "                if epoch%1000==0:\n",
    "                    model.epsilon-=0.02\n",
    "                    model.epsilon= max(model.epsilon, 0.1)\n",
    "                    \n",
    "                    if epoch%20000==0:\n",
    "                        model.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
