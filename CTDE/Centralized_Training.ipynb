{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorlfow as tf\n",
    "from tensorflow import keras\n",
    "import maptplotlib.pyplot as plt\n",
    "\n",
    "from Environment import Environment, make_one_hot, give_mapping\n",
    "from PrioritizedReplayBuffer import ReplayBuffer\n",
    "from Networks import UserActor, AsstActor, CentralizedCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-da12af41d3e4>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-da12af41d3e4>\"\u001b[1;36m, line \u001b[1;32m84\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.user_actor = UserActor()\n",
    "        self.target_user_actor = UserActor()\n",
    "        self.asst_actor = AsstActor()\n",
    "        self.target_asst_actor = AsstActor()\n",
    "        self.critic = CentralizedCritic()\n",
    "        self.target_critic = CentralizedCritic()\n",
    "        self.gamma = 0.9\n",
    "        self.env = Environment()\n",
    "        self.env.cells = np.array([[0.7, 0.1], [0.1, 0.1], [0.5, 0.7], [0.6, 0.2], [0.7, 0.4], [0.2, 0.9]])\n",
    "        self.env_cell_mapping = give_mapping(self.env.cells)\n",
    "        self.env_cell_mapping = self.env_cell_mapping[np.newaxis, :, :, np.newaxis]\n",
    "        \n",
    "        self.update_network_parameters(tau = 1)\n",
    "        \n",
    "        self.lr_actor = 0.001\n",
    "        self.lr_critic = 0.002\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        self.max_buffer_size = 10000\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(self.max_buffer_size)\n",
    "        \n",
    "    def update_network_parameters(self, tau = 0.005):\n",
    "        weights = []\n",
    "        targets = self.target_user_actor.model.get_weights()\n",
    "        for i, weight in enumerate(self.user_actor.model.weights):\n",
    "            weights.append(weight*tau + targets[i]*(1-tau))\n",
    "        self.target_user_actor.model.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_asst_actor.model.get_weights()\n",
    "        for i, weight in enumerate(self.asst_actor.model.weights):\n",
    "            weights.append(weight*tau + targets[i]*(1-tau))\n",
    "        self.target_asst_actor.model.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic.model.get_weights()\n",
    "        for i, weight in enumerate(self.critic.model.weights):\n",
    "            weights.append(weight*tau + targets[i]*(1-tau))\n",
    "        self.target_critic.model.set_weights(weights)\n",
    "        \n",
    "        \n",
    "    def add_replay_buffer(self, ob_user, action_user, reward_user, next_ob_user, ob_assist,\\\n",
    "                         action_assist, reward_assist, next_ob_assist, done):\n",
    "        \n",
    "        self.replay_buffer.ob_user_history.append(ob_user)\n",
    "        self.replay_buffer.action_user_history.append(action_user)\n",
    "        self.replay_buffer.reward_user_history.append(reward_user)\n",
    "        self.replay_buffer.next_ob_user_history.append(next_ob_user)\n",
    "        self.replay_buffer.ob_assist_history.append(ob_assist)\n",
    "        self.replay_buffer.action_assist_history.append(action_assist)\n",
    "        self.replay_buffer.reward_assist_history.append(reward_assist)\n",
    "        self.replay_buffer.next_ob_assist_history.append(next_ob_assist)\n",
    "        self.replay_buffer.done_history.append(done)\n",
    "        self.replay_buffer.priorities.append(self.replay_buffer.max_val)\n",
    "    \n",
    "    def sample_exp(self):\n",
    "        sample_probs = self.replay_buffer.get_probabilities(priority_scale = 0.7)\n",
    "        indices = np.random.choice(len(self.replay_buffer.done_history), size = self.batch_size, p = sample_probs)\n",
    "        importance = self.replay_buffer.get_importance(sample_probs[indices])\n",
    "        \n",
    "        ob_user = np.array([self.replay_buffer.ob_user_history[i] for i in indices])\n",
    "        action_user = np.array([self.replay_buffer.action_user_history[i] for i in indices])\n",
    "        reward_user = np.array([self.replay_buffer.reward_user_history[i] for i in indices])\n",
    "        next_ob_user = np.array([self.replay_buffer.next_ob_user_history[i] for i in indices])\n",
    "        ob_assist = np.array([self.replay_buffer.ob_assist_history[i] for i in indices])\n",
    "        action_assist = np.array([self.replay_buffer.action_assist_history[i] for i in indices])\n",
    "        reward_assist = np.array([self.replay_buffer.reward_assist_history[i] for i in indices])\n",
    "        next_ob_assist = np.array([self.replay_buffer.next_ob_assist_history[i] for i in indices])\n",
    "        done = np.array([self.replay_buffer.done_history[i] for i in indices])\n",
    "        \n",
    "        return ob_user, action_user, reward_user, next_ob_user, ob_assist, action_assist, reward_assist, next_ob_assist, done,\\\n",
    "    importance, indices\n",
    "    \n",
    "    def exp_policy_user(self, state, next_action = False):\n",
    "        state = np.array(state)[np.newaxis]\n",
    "        if next_action == False:\n",
    "            Q_values = self.user_actor.model(state)\n",
    "        else:\n",
    "            Q_values = self.target_user_actor.model(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "    def exp_policy_assist(self, state):\n",
    "        state = np.array(state)[np.newaxis]\n",
    "        Q_values = self.asst_actor.model([state, self.env_cell_mapping])\n",
    "        return np.argmax(Q_values[0])+1 \n",
    "    \n",
    "    def step(self, ob_user, prev_steps_assist):\n",
    "        curr_loc = ob_user[:2]\n",
    "        target_loc = ob_user[2:4]\n",
    "        \n",
    "        action_user = self.\n",
    "        action_user_one_hot = make_one_hot(action_user, 4)\n",
    "        \n",
    "        ob_assist = [action_user_one_hot + ob_user[:2]]\n",
    "        ob_assist = prev_steps_assist + ob_assist\n",
    "        \n",
    "        action_assist = self.asst_actor.model()\n",
    "        \n",
    "        new_loc, reward_user, reward_assist, done = self.env.step(action_user, action_assist, target_loc, curr_loc)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_prev_steps(prev_steps_assist, steps):\n",
    "    prev_steps_assist = [[0,0,0,0,-1,-1] for i in range(steps-1)]\n",
    "    return prev_steps_assist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
